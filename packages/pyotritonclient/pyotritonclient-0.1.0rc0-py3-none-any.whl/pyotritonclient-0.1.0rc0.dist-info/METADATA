Metadata-Version: 2.1
Name: pyotritonclient
Version: 0.1.0rc0
Summary: A Pyodide python http client library and utilities for communicating with Triton Inference Server (based on tritonclient from NVIDIA)
Home-page: https://developer.nvidia.com/nvidia-triton-inference-server
Author: Wei OUYANG
Author-email: oeway007@gmail.com
License: BSD
Keywords: http,triton,tensorrt,inference,server,service,client,nvidia
Platform: UNKNOWN
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Intended Audience :: Information Technology
Classifier: Topic :: Scientific/Engineering
Classifier: Topic :: Scientific/Engineering :: Image Recognition
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries
Classifier: Topic :: Utilities
Classifier: License :: OSI Approved :: BSD License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Environment :: Console
Classifier: Natural Language :: English
Classifier: Operating System :: OS Independent
Description-Content-Type: text/markdown
License-File: LICENSE.txt
Requires-Dist: six
Requires-Dist: numpy (>=1.19.1)

This is a simplified implemetation of the triton client from NVIDIA, it is mainly made for running in the web browser with pyodide.It only implement the http client, and most of the API remains the same but changed into async.For example, instead of doing `client.infer(...)`, you need to do `await client.infer(...)`.See [download-using-python-package-installer-pip](https://github.com/triton-inference-server/client/tree/main#download-using-python-package-installer-pip) for package details.

The [client examples](https://github.com/triton-inference-server/client/tree/main/src/python/examples) demonstrate how to use the package to issue request to [triton inference server](https://github.com/triton-inference-server/server).

