# coding=utf-8
from multiprocessing.pool import ThreadPool
from threading import Thread
import itertools
import time
import multiprocessing
from typing import Callable, Tuple, Union

#################################################################################################
def log(*s): print(*s, flush=True)



#################################################################################################
def test_parallel():
    import pandas as pd
    def fun_async(xlist):
        list = []
        for x in xlist:
            stdr = ""
            for y in x:
                stdr += y
            list.append(stdr)
        return list

    def group_function(name, group):         # Inverse cumulative sum
       group["inv_sum"] = group.iloc[::-1]["value"].cumsum()[::-1].shift(-1).fillna(0)
       return group

    def apply_func(x):
       return x ** 2

    #### multithread_run
    li_of_tuples = [("x", "y", "z"),("y", "z", "p"),("yw", "zs", "psd"),("yd", "zf", "pf")]
    res =  multithread_run(fun_async, li_of_tuples, npool=2, start_delay=0.1, verbose=True)
    print([["xyz", "ywzspsd"], ["yzp", "ydzfpf"]]== res )


    #### multiproc_run
    li_of_tuples = [("x", "y", "z"),("y", "z", "p"),("yw", "zs", "psd"),("yd", "zf", "pf"),]
    res = multiproc_run(fun_async, li_of_tuples, npool=2, start_delay=0.1, verbose=True)
    print( res == [["xyz"], ["yzp"],
    ["ywzspsd"], ["ydzfpf"], []])


    #### pd_groupby_parallel
    df = pd.DataFrame(data={'result':[5, 8, 1, 7, 0, 3, 2, 9, 4, 6],
                            'user_id':[1, 1, 2, 3, 4, 4, 5, 8, 9, 9],
                            'value'  :[27, 14, 26, 19, 28, 9, 11, 1, 26, 18],'data_chunk':[1, 1, 2, 3, 4, 4, 5, 8, 9, 9]})
    expected_df = df.copy()
    expected_df["inv_sum"] = [14.0, 0.0, 0.0, 0.0, 9.0, 0.0, 0.0, 0.0, 18.0, 0.0]
    result = pd_groupby_parallel(df.groupby("user_id"), func=group_function, npool=5)
    print(expected_df.equals(result))


    ### pd_apply_parallel2
    df = pd.DataFrame({"A": [0, 1, 2, 3, 4],   "B": [100, 200, 300, 400, 500],})
    expected_df = pd.DataFrame({"A": [0, 1, 4, 9, 16], "B": [10000, 40000, 90000, 160000, 250000]})
    result = pd_groupby_parallel2(df=df, colsgroup=["A" "B"], fun_apply=apply_func, npool=4)
    print(expected_df.equals(result))



########################################################################################################
def pd_groupby_parallel(groupby_df,func=None,npool: int = 5,**kw,):
    """Performs a Pandas groupby operation in parallel.
    pd.core.groupby.DataFrameGroupBy
    Example usage:
        import pandas as pd
        df = pd.DataFrame({'A': [0, 1], 'B': [100, 200]})
        df.groupby(df.groupby('A'), lambda row: row['B'].sum())
    Authors: Tamas Nagy and Douglas Myers-Turnbull
    """
    import pandas as pd

    num_cpus = multiprocessing.cpu_count() - 1 if npool == -1 else npool
    with multiprocessing.Pool(num_cpus) as pool:
        queue  = multiprocessing.Manager().Queue()
        result = pool.starmap_async(func, [(name, group) for name, group in groupby_df])
        cycler = itertools.cycle("\|/â€•")
        while not result.ready():
            log( "Percent complete: {:.0%} {}".format( queue.qsize() / len(groupby_df), next(cycler) ))
            time.sleep(0.4)
        got = result.get(timeout=1)
    # log("\nProcessed {} rows in {:.1f}s".format(len(got), time.time() - start))
    return pd.concat(got)



def pd_groupby_parallel2(df,colsgroup=None,fun_apply=None,npool=5,start_delay=0.01, verbose=0):
    """Pandas parallel apply"""
    import pandas as pd, numpy as np, time, gc

    dfg = df.groupby(colsgroup)  ### Need to get the splits
    def f2(df_list):
        dfiall = None
        for dfi in df_list:
            dfi    = dfi.apply(fun_apply)
            dfiall = pd.concat((dfiall, dfi)) if dfiall is None else dfi
            del dfi ;  gc.collect()
        return dfiall

    #### Pool execute #################################################
    import multiprocessing as mp
    # pool     = multiprocessing.Pool(processes=npool)
    pool       = mp.pool.ThreadPool(processes=npool)
    job_list   = []
    input_list = [[] * npool]

    for i, name, dfi in enumerate(dfg):
        input_list[i % npool].append(dfg)

    for i, inputi in enumerate(input_list):
        time.sleep(start_delay)
        log("starts", i)
        job_list.append(pool.apply_async(f2, (inputi,)))
        if verbose>0:
            log(i, dfi.shape)

    ##### Aggregate results ##########################################
    dfall = None
    for i in range(npool):
        if i >= len(job_list):
            break
        dfi   = job_list[i].get()
        dfall = pd.concat((dfall, dfi)) if dfall is not None else dfi
        del dfi
        log(i, "job finished")

    pool.terminate() ; pool.join() ; pool = None
    return dfall


def pd_apply_parallel(df, colsgroup=None, fun_apply=None, npool=5, start_delay=0.01, verbose=True):
    """Pandas parallel apply"""
    import pandas as pd, numpy as np, time, gc

    def f2(df):
        return df.apply(fun_apply, axis=1)

    size = int(len(df) // npool)

    #### Pool execute ###################################
    import multiprocessing as mp
    # pool     = multiprocessing.Pool(processes=npool)
    pool     = mp.pool.ThreadPool(processes=npool)
    job_list = []

    for i in range(npool):
        time.sleep(start_delay)
        log("starts", i)
        i2 = i + 2 if i == npool - 1 else i + 1
        dfi = df.iloc[i * size : (i2 * size), :]
        job_list.append(pool.apply_async(f2, (dfi,)))
        if verbose:
            log(i, dfi.shape)

    dfall = None
    for i in range(npool):
        if i >= len(job_list):
            break
        dfi = job_list[i].get()
        dfall = pd.concat((dfall, dfi)) if dfall is not None else dfi
        del dfi
        log(i, "job finished")

    pool.terminate() ; pool.join() ; pool = None
    return dfall


def multiproc_run(fun_async, input_list: list, npool=5, start_delay=0.1, verbose=True, **kw):
    """Multiprocessing execute
    input is as list of tuples  [(x1,x2,x3), (y1,y2,y3) ]
    def fun_async(xlist):
      for x in xlist :
            download.upload(x[0], x[1])
    """
    import time
    #### Input xi #######################################
    xi_list = [[] for t in range(npool)]
    for i, xi in enumerate(input_list):
        jj = i % npool
        xi_list[jj].append(tuple(xi))

    if verbose:
        for j in range(len(xi_list)):
            print("thread ", j, len(xi_list[j]))
        time.sleep(6)

    #### Pool execute ###################################
    import multiprocessing

    pool = multiprocessing.Pool(processes=npool)
    # pool   = mp.pool.ThreadPool(processes=npool)
    job_list = []
    for i in range(npool):
        time.sleep(start_delay)
        log("starts", i)
        job_list.append(pool.apply_async(fun_async, (xi_list[i],)))
        if verbose:
            log(i, xi_list[i])

    res_list = []
    for i in range(npool):
        if i >= len(job_list):
            break
        res_list.append(job_list[i].get())
        log(i, "job finished")

    pool.terminate() ; pool.join() ; pool = None
    log("n_processed", len(res_list))
    return res_list




def multithread_run(fun_async, input_list: list, npool=2, start_delay=0.1, verbose=True, **kw):
    """input is as list of tuples  [(x1,x2,x3), (y1,y2,y3) ]
    def fun_async(xlist):
      for x in xlist :
            hdfs.upload(x[0], x[1])
    """
    import time


    result_list = []
    def log_result(result):
        # This is called whenever foo_pool(i) returns a result.
        # result_list is modified only by the main process, not the pool workers.
        result_list.append(result)

    #### Input xi #######################################
    xi_list = [[] for t in range(npool)]
    for i, xi in enumerate(input_list):
        jj = i % npool
        xi_list[jj].append(tuple(xi))
    if verbose:
        for j in range(len(xi_list)):
            print("thread ", j, len(xi_list[j]))
        time.sleep(6)

    #### Pool execute ###################################
    import multiprocessing as mp
    # pool     = multiprocessing.Pool(processes=3)
    pool = mp.pool.ThreadPool(processes=npool)
    job_list = []
    for i in range(npool):
        time.sleep(start_delay)
        log("starts", i)
        job_list.append(pool.apply_async(fun_async, (xi_list[i],), callback=log_result))
        if verbose:
            log(i, xi_list[i])

    res_list = []
    for i in range(npool):
        if i >= len(job_list):
            break
        res_list.append(job_list[i].get())
        log(i, "job finished")

    pool.terminate() ; pool.join() ; pool = None
    log("n_processed", len(res_list))
    return res_list


def multithread_run_list(**kwargs):
    """Creating n number of threads:  1 thread per function,    starting them and waiting for their subsequent completion
    os_multithread(function1=(test_print, ("some text",)),
                          function2=(test_print, ("bbbbb",)),
                          function3=(test_print, ("ccccc",)))
    """
    class ThreadWithResult(Thread):
        def __init__(  self,  group=None, target=None,  name=None,  args=(),  kwargs={},  *,  daemon=None,  ):
            self.result = None
            def function():
                self.result = target(*args, **kwargs)
            super().__init__(group=group, target=function, name=name, daemon=daemon)

    list_of_threads = []
    for thread in kwargs.values():
        t = ThreadWithResult(target=thread[0], args=thread[1])
        list_of_threads.append(t)

    for thread in list_of_threads:
        thread.start()

    results = []
    for thread, keys in zip(list_of_threads, kwargs.keys()):
        thread.join()
        results.append((keys, thread.result))

    return results


if __name__ == '__main__':
    test_parallel()


