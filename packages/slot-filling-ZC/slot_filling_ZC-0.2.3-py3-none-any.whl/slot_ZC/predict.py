# -*- coding: utf-8 -*-
"""predict.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DWv_OA1CWWxqZzw6ETVS1NkcZOL3TVqT
"""

# from google.colab import drive
# drive.mount('/content/drive')

import sys
import os
#%%
#load the model
import torch
import numpy as np
import random
import torch 
import torch.nn as nn
import torch.nn.functional as F
from torch import optim

train_file = "data/train_dev"
test_file = "data/test"
vocab_intent_file = "data/vocab.intent"
vocab_slot_file = "data/vocab.slot"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
total_epoch = 50
max_len = 50
batch = 16
learning_rate = 0.001
DROPOUT = 0.2
embedding_size = 300
lstm_hidden_size = 200

#make dicts
def convert_int(arr):
    try:
        a = int(arr)
    except:
        return None
    return a
# Make words dict
def make_word_dict(train_file):
    #create a word list	
    words = []
    with open(train_file) as f:
        for line in f.readlines():
            line = line.strip().lower().split()

            for index, item in enumerate(line):
                word = item.split(':')[0]
                if word == '<=>':
                    break
                if convert_int(word) is not None:
                    words.append('DIGIT' * len(word))
                else:        
                    words.append(word)
    #create a word dict
    words_vocab = sorted(set(words))
    word_dict = {'UNK': 0, 'PAD': 1}
    for i, item in enumerate(words_vocab):
        word_dict[item] = i + 2
    return word_dict


# Make slot tag dict 
def make_slot_dict(vocab_slot_file):
    slot_dict = {}
    with open(vocab_slot_file) as f:
        for i, line in enumerate(f.readlines()):
            slot_dict[line.strip()] = i
    return slot_dict


#make intent dict
def make_intent_dict(vocab_intent_file):
    intent_dict = {}
    with open(vocab_intent_file) as f:
        for i, line in enumerate(f.readlines()):
            intent_dict[line.strip()] = i
    return intent_dict

word_dict=make_word_dict(train_file)
slot_dict=make_slot_dict(vocab_slot_file)
intent_dict=make_intent_dict(vocab_intent_file)
num_intent=len(intent_dict)
#Model
# Bi-model 
class slot_enc(nn.Module):
    def __init__(self, embedding_size, lstm_hidden_size, vocab_size=len(word_dict)):
        super(slot_enc, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_size).to(device)
        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=lstm_hidden_size, num_layers=2,\
                            bidirectional= True, batch_first=True) #, dropout=DROPOUT)

    def forward(self, x):
        x = self.embedding(x)
        x = F.dropout(x, DROPOUT)       
        x, _ = self.lstm(x)
        x = F.dropout(x, DROPOUT)
        return x 


class slot_dec(nn.Module):
    def __init__(self, lstm_hidden_size, label_size=len(slot_dict)):
        super(slot_dec, self).__init__()
        self.lstm = nn.LSTM(input_size=lstm_hidden_size*5, hidden_size=lstm_hidden_size, num_layers=1)
        self.fc = nn.Linear(lstm_hidden_size, label_size)
        self.hidden_size = lstm_hidden_size

    def forward(self, x, hi):
        batch = x.size(0)
        length = x.size(1)
        dec_init_out = torch.zeros(batch, 1, self.hidden_size).to(device)
        hidden_state = (torch.zeros(1, 1, self.hidden_size).to(device), \
                        torch.zeros(1, 1, self.hidden_size).to(device))
        x = torch.cat((x, hi), dim=-1)

        x = x.transpose(1, 0)  # 50 x batch x feature_size
        x = F.dropout(x, DROPOUT)
        all_out = []
        for i in range(length):
            if i == 0:
                out, hidden_state = self.lstm(torch.cat((x[i].unsqueeze(1), dec_init_out), dim=-1), hidden_state)
            else:
                out, hidden_state = self.lstm(torch.cat((x[i].unsqueeze(1), out), dim=-1), hidden_state)
            all_out.append(out)
        output = torch.cat(all_out, dim=1) # 50 x batch x feature_size
        x = F.dropout(x, DROPOUT)
        res = self.fc(output)
        return res 

class intent_enc(nn.Module):
    def __init__(self, embedding_size, lstm_hidden_size, vocab_size=len(word_dict)):
        super(intent_enc, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_size).to(device)
        # self.embedding.weight.data.uniform_(-1.0, 1.0)
        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size= lstm_hidden_size, num_layers=2,\
                            bidirectional= True, batch_first=True, dropout=DROPOUT)
    
    def forward(self, x):
        x = self.embedding(x)
        x = F.dropout(x, DROPOUT)
        x, _ = self.lstm(x)
        x = F.dropout(x, DROPOUT)
        return x

class intent_dec(nn.Module):
    def __init__(self, lstm_hidden_size, label_size=len(intent_dict)):
        super(intent_dec, self).__init__()
        self.lstm = nn.LSTM(input_size=lstm_hidden_size*4, hidden_size=lstm_hidden_size, batch_first=True, num_layers=1)#, dropout=DROPOUT)
        self.fc = nn.Linear(lstm_hidden_size, label_size)
        
    def forward(self, x, hs, real_len):
        batch = x.size()[0]
        real_len = torch.tensor(real_len).to(device)
        x = torch.cat((x, hs), dim=-1)
        x = F.dropout(x, DROPOUT)
        x, _ = self.lstm(x)
        x = F.dropout(x, DROPOUT)

        index = torch.arange(batch).long().to(device)
        state = x[index, real_len-1, :]
        
        res = self.fc(state.squeeze())
        return res
        
class Intent(nn.Module):
    def __init__(self):
        super(Intent, self).__init__()
        self.enc = intent_enc(embedding_size, lstm_hidden_size).to(device)
        self.dec = intent_dec(lstm_hidden_size).to(device)
        self.share_memory = torch.zeros(batch, max_len, lstm_hidden_size * 2).to(device)
    

class Slot(nn.Module):
    def __init__(self):
        super(Slot, self).__init__()
        self.enc = slot_enc(embedding_size, lstm_hidden_size).to(device)
        self.dec = slot_dec(lstm_hidden_size).to(device)
        self.share_memory = torch.zeros(batch, max_len, lstm_hidden_size * 2).to(device)

slot_model = Slot().to(device)
intent_model = Intent().to(device) 


slot_model.load_state_dict(torch.load('model_slot_best.pt'), strict=False)

intent_model.load_state_dict(torch.load('model_intent_best.pt'), strict=False)


#%%
#useful functions from utils
def get_batch(data, batch_size=batch):
        random.shuffle(data)
        sindex = 0
        eindex = batch_size
        while eindex < len(data):

            sentence = []
            real_len = []
            slot_label = []
            intent_label = []
            
            batch = data[sindex:eindex]
            for m in range(sindex, eindex):
                sentence.append(data[m][0])
                real_len.append(data[m][1])
                slot_label.append(data[m][2])
                intent_label.append(data[m][3])

            temp = eindex
            eindex = eindex + batch_size
            sindex = temp

            yield (sentence, real_len, slot_label, intent_label)

def makeindex(filename):
        train_data = []
        with open(filename) as f:
            for line in f.readlines():
                line = line.strip().split()
                sample_sentence = []
                sample_slot = []
                for index, item in enumerate(line):
                    word = item.split(':')[0] 

                    if word == '<=>':
                        real_length = index
                        break
                    if convert_int(word) is not None:
                        word =  'DIGIT' * len(word)
                    else:
                        pass
                    slot = item.rsplit(':',1)[1]

                    if word in word_dict:
                        sample_sentence.append(word_dict[word])
                    else:
                        sample_sentence.append(word_dict['UNK'])

                    sample_slot.append(slot_dict[slot])

                    train_intent = intent_dict[ line[-1].split(';')[0] ]

                while len(sample_sentence) < max_len:
                    sample_sentence.append(word_dict['PAD'])
                while len(sample_slot) < max_len:
                    sample_slot.append(slot_dict['O'])

                train_data.append([sample_sentence, real_length, sample_slot, train_intent])
        return train_data

def make_mask(real_len, max_len=max_len, label_size=len(slot_dict), batch=batch):
        mask = torch.zeros(batch, max_len, label_size)
        for index, item in enumerate(real_len):
            mask[index, :item, :] = 1.0
        return mask

def masked_log_softmax(vector: torch.Tensor, mask: torch.Tensor, dim: int = -1) -> torch.Tensor:
        if mask is not None:
            mask = mask.float()
            while mask.dim() < vector.dim():
                mask = mask.unsqueeze(1)

            vector = vector + (mask + 1e-45).log()
        return torch.nn.functional.log_softmax(vector, dim=dim)

def get_chunks(labels):
        chunks = []
        start_idx,end_idx = 0,0
        for idx in range(1,len(labels)-1):
            chunkStart, chunkEnd = False,False
            if labels[idx-1] not in ('O', '<pad>', '<unk>', '<s>', '</s>', '<STOP>', '<START>'):
                prevTag, prevType = labels[idx-1][:1], labels[idx-1][2:]
            else:
                prevTag, prevType = 'O', 'O'
            if labels[idx] not in ('O', '<pad>', '<unk>', '<s>', '</s>', '<STOP>', '<START>'):
                Tag, Type = labels[idx][:1], labels[idx][2:]
            else:
                Tag, Type = 'O', 'O'
            if labels[idx+1] not in ('O', '<pad>', '<unk>', '<s>', '</s>', '<STOP>', '<START>'):
                nextTag, nextType = labels[idx+1][:1], labels[idx+1][2:]
            else:
                nextTag, nextType = 'O', 'O'

            if (Tag == 'B' and prevTag in ('B', 'I', 'O')) or (prevTag, Tag) in [('O', 'I'), ('E', 'E'), ('E', 'I'), ('O', 'E')]:
                chunkStart = True
            if Tag != 'O' and prevType != Type:
                chunkStart = True

            if (Tag in ('B','I') and nextTag in ('B','O')) or (Tag == 'E' and nextTag in ('E', 'I', 'O')):
                chunkEnd = True
            if Tag != 'O' and Type != nextType:
                chunkEnd = True

            if chunkStart:
                start_idx = idx
            if chunkEnd:
                end_idx = idx
                chunks.append((start_idx,end_idx,Type))
                start_idx,end_idx = 0,0
        return chunks

#%%predict

test_data = makeindex(test_file)
a=get_batch(test_data, batch_size=1)
#choose 1 sentence as input
data_test=next(a)

sentence_test, real_len_test, slot_label_test, intent_label_test = data_test
#x_test is the sentence_test, encoded sentences
x_test = torch.tensor(sentence_test).to(device)

mask_test = make_mask(real_len_test, batch=1).to(device)
# Slot model generate hs_test and intent model generate hi_test
hs_test = slot_model.enc(x_test)
hi_test = intent_model.enc(x_test)

#%%
# Slot
slot_logits_test = slot_model.dec(hs_test, hi_test)
log_slot_logits_test = masked_log_softmax(slot_logits_test, mask_test, dim=-1)
slot_pred_test = torch.argmax(log_slot_logits_test, dim=-1)

index2slot_dict = {}
for key in slot_dict:
    index2slot_dict[slot_dict[key]] = key
#slot_pred_test
#slot_label_test
slot_pred_test = slot_pred_test[0][:real_len_test[0]]
slot_pred_test = [int(item) for item in slot_pred_test]
slot_pred_test = [index2slot_dict[item] for item in slot_pred_test]

slot_pred_test
#%%
pred_chunks = get_chunks(['O'] + slot_pred_test + ['O'])
pred_chunks

#%%
import torch.nn.functional as F
# Intent
intent_logits_test = intent_model.dec(hi_test, hs_test, real_len_test)
log_intent_logits_test = F.log_softmax(intent_logits_test, dim=-1)
res_test = torch.argmax(log_intent_logits_test, dim=-1)

res_test
#%%
#prepare predict input
s_input="find nonstop flights from salt lake city to new york on saturday april ninth."


#%%

def makeoneindex(line):
    predict_data = []
    line = line.strip().split()
    sample_sentence = []
    real_length=0
    for index, item in enumerate(line):
        real_length+=1
        word = item.split(' ')[0] 
        if convert_int(word) is not None:
            word =  'DIGIT' * len(word)
        else:
            pass

        if word in word_dict:
            sample_sentence.append(word_dict[word])
        else:
            sample_sentence.append(word_dict['UNK'])

    while len(sample_sentence) < max_len:
        sample_sentence.append(word_dict['PAD'])
    predict_data.append(sample_sentence)
    return predict_data

#%%
sentence_predict=makeoneindex(s_input)
real_len_predict=[len(s_input.split())]

#%%
#x_test is the sentence_test, encoded sentences
x_predict = torch.tensor(sentence_predict).to(device)

mask_predict = make_mask(real_len_predict, batch=1).to(device)
# Slot model generate hs_test and intent model generate hi_test
hs_predict = slot_model.enc(x_predict)
hi_predict = intent_model.enc(x_predict)

sentence_predict
# %%
#slot
slot_logits_predict = slot_model.dec(hs_predict, hi_predict)
log_slot_logits_predict = masked_log_softmax(slot_logits_predict, mask_predict, dim=-1)
slot_pred_predict = torch.argmax(log_slot_logits_predict, dim=-1)

slot_pred_predict = slot_pred_predict[0][:real_len_test[0]]
slot_pred_predict = [int(item) for item in slot_pred_predict]
slot_pred_predict = [index2slot_dict[item] for item in slot_pred_predict]

#chunk
pred_chunks = get_chunks(['O'] + slot_pred_predict + ['O'])
pred_chunks
# %%
#intent
intent_logits_predict = intent_model.dec(hi_predict, hs_predict, real_len_predict)
log_intent_logits_predict = F.log_softmax(intent_logits_predict, dim=-1)
res_predict = torch.argmax(log_intent_logits_predict, dim=-1)

res_predict

for intent, num in intent_dict.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)
    if res_predict == num:
        intent_predict=intent

intent_predict
# %%
def predict(sentence,
            slot_model_path="model_slot_best.pt",
            intent_model_path="model_intent_best.pt",
            train_file = "data/train_dev",
            test_file = "data/test",
            vocab_intent_file = "data/vocab.intent",
            vocab_slot_file = "data/vocab.slot",
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu"),
            total_epoch = 50,
            max_len = 50,
            batch = 16,
            learning_rate = 0.001,
            DROPOUT = 0.2,
            embedding_size = 300,
            lstm_hidden_size = 200):
    #make dicts
    word_dict=make_word_dict(train_file)
    slot_dict=make_slot_dict(vocab_slot_file)
    intent_dict=make_intent_dict(vocab_intent_file)
    num_intent=len(intent_dict)
    #make index of sentence
    def makeoneindex(line):
        predict_data = []
        line = line.strip().split()
        sample_sentence = []
        real_length=0
        for index, item in enumerate(line):
            real_length+=1
            word = item.split(' ')[0] 
            if convert_int(word) is not None:
                word =  'DIGIT' * len(word)
            else:
                pass

            if word in word_dict:
                sample_sentence.append(word_dict[word])
            else:
                sample_sentence.append(word_dict['UNK'])

        while len(sample_sentence) < max_len:
            sample_sentence.append(word_dict['PAD'])
        predict_data.append(sample_sentence)
        return predict_data
    #prepare for prediction
    sentence_predict=makeoneindex(sentence)
    real_len_predict=[len(sentence.split())]

    #load model
    # Bi-model 
    class slot_enc(nn.Module):
        def __init__(self, embedding_size, lstm_hidden_size, vocab_size=len(word_dict)):
            super(slot_enc, self).__init__()

            self.embedding = nn.Embedding(vocab_size, embedding_size).to(device)
            self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=lstm_hidden_size, num_layers=2,\
                                bidirectional= True, batch_first=True) #, dropout=DROPOUT)

        def forward(self, x):
            x = self.embedding(x)
            x = F.dropout(x, DROPOUT)       
            x, _ = self.lstm(x)
            x = F.dropout(x, DROPOUT)
            return x 


    class slot_dec(nn.Module):
        def __init__(self, lstm_hidden_size, label_size=len(slot_dict)):
            super(slot_dec, self).__init__()
            self.lstm = nn.LSTM(input_size=lstm_hidden_size*5, hidden_size=lstm_hidden_size, num_layers=1)
            self.fc = nn.Linear(lstm_hidden_size, label_size)
            self.hidden_size = lstm_hidden_size

        def forward(self, x, hi):
            batch = x.size(0)
            length = x.size(1)
            dec_init_out = torch.zeros(batch, 1, self.hidden_size).to(device)
            hidden_state = (torch.zeros(1, 1, self.hidden_size).to(device), \
                            torch.zeros(1, 1, self.hidden_size).to(device))
            x = torch.cat((x, hi), dim=-1)

            x = x.transpose(1, 0)  # 50 x batch x feature_size
            x = F.dropout(x, DROPOUT)
            all_out = []
            for i in range(length):
                if i == 0:
                    out, hidden_state = self.lstm(torch.cat((x[i].unsqueeze(1), dec_init_out), dim=-1), hidden_state)
                else:
                    out, hidden_state = self.lstm(torch.cat((x[i].unsqueeze(1), out), dim=-1), hidden_state)
                all_out.append(out)
            output = torch.cat(all_out, dim=1) # 50 x batch x feature_size
            x = F.dropout(x, DROPOUT)
            res = self.fc(output)
            return res 

    class intent_enc(nn.Module):
        def __init__(self, embedding_size, lstm_hidden_size, vocab_size=len(word_dict)):
            super(intent_enc, self).__init__()
            
            self.embedding = nn.Embedding(vocab_size, embedding_size).to(device)
            # self.embedding.weight.data.uniform_(-1.0, 1.0)
            self.lstm = nn.LSTM(input_size=embedding_size, hidden_size= lstm_hidden_size, num_layers=2,\
                                bidirectional= True, batch_first=True, dropout=DROPOUT)
        
        def forward(self, x):
            x = self.embedding(x)
            x = F.dropout(x, DROPOUT)
            x, _ = self.lstm(x)
            x = F.dropout(x, DROPOUT)
            return x

    class intent_dec(nn.Module):
        def __init__(self, lstm_hidden_size, label_size=len(intent_dict)):
            super(intent_dec, self).__init__()
            self.lstm = nn.LSTM(input_size=lstm_hidden_size*4, hidden_size=lstm_hidden_size, batch_first=True, num_layers=1)#, dropout=DROPOUT)
            self.fc = nn.Linear(lstm_hidden_size, label_size)
            
        def forward(self, x, hs, real_len):
            batch = x.size()[0]
            real_len = torch.tensor(real_len).to(device)
            x = torch.cat((x, hs), dim=-1)
            x = F.dropout(x, DROPOUT)
            x, _ = self.lstm(x)
            x = F.dropout(x, DROPOUT)

            index = torch.arange(batch).long().to(device)
            state = x[index, real_len-1, :]
            
            res = self.fc(state.squeeze())
            return res
            
    class Intent(nn.Module):
        def __init__(self):
            super(Intent, self).__init__()
            self.enc = intent_enc(embedding_size, lstm_hidden_size).to(device)
            self.dec = intent_dec(lstm_hidden_size).to(device)
            self.share_memory = torch.zeros(batch, max_len, lstm_hidden_size * 2).to(device)
        

    class Slot(nn.Module):
        def __init__(self):
            super(Slot, self).__init__()
            self.enc = slot_enc(embedding_size, lstm_hidden_size).to(device)
            self.dec = slot_dec(lstm_hidden_size).to(device)
            self.share_memory = torch.zeros(batch, max_len, lstm_hidden_size * 2).to(device)

    slot_model = Slot().to(device)
    intent_model = Intent().to(device) 


    slot_model.load_state_dict(torch.load(slot_model_path), strict=False)

    intent_model.load_state_dict(torch.load(intent_model_path), strict=False)
    #predict
    x_predict = torch.tensor(sentence_predict).to(device)
    mask_predict = make_mask(real_len_predict, batch=1).to(device)
    # Slot model generate hs_predict and intent model generate hi_predict
    hs_predict = slot_model.enc(x_predict)
    hi_predict = intent_model.enc(x_predict)
    #slot
    slot_logits_predict = slot_model.dec(hs_predict, hi_predict)
    log_slot_logits_predict = masked_log_softmax(slot_logits_predict, mask_predict, dim=-1)
    slot_pred_predict = torch.argmax(log_slot_logits_predict, dim=-1)

    slot_pred_predict = slot_pred_predict[0][:real_len_test[0]]
    slot_pred_predict = [int(item) for item in slot_pred_predict]
    slot_pred_predict = [index2slot_dict[item] for item in slot_pred_predict]

    #chunk
    pred_chunks = get_chunks(['O'] + slot_pred_predict + ['O'])

    #intent
    intent_logits_predict = intent_model.dec(hi_predict, hs_predict, real_len_predict)
    log_intent_logits_predict = F.log_softmax(intent_logits_predict, dim=-1)
    res_predict = torch.argmax(log_intent_logits_predict, dim=-1)
    for intent, num in intent_dict.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)
        if res_predict == num:
            intent_predict=intent
    return pred_chunks,intent_predict

#%%
import pickle
#save dictionaries
# Store data (serialize)
with open('word_dict.pickle', 'wb') as handle:
    pickle.dump(word_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)

with open('slot_dict.pickle', 'wb') as handle:
    pickle.dump(slot_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)

with open('intent_dict.pickle', 'wb') as handle:
    pickle.dump(intent_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)

# %%
#load dictionaries
with open('word_dict.pickle', 'rb') as handle:
    word_dict = pickle.load(handle)

with open('slot_dict.pickle', 'rb') as handle:
    slot_dict = pickle.load(handle)

with open('intent_dict.pickle', 'rb') as handle:
    intent_dict = pickle.load(handle)
# %%
